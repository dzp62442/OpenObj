## 1 ç¯å¢ƒé…ç½®

```shell
# ä»…åŒ…å«åŸºæœ¬ç¯å¢ƒï¼Œè¿˜éœ€è¦æ‰‹åŠ¨å®‰è£…åé¢çš„ CropFormerã€TAP å’Œ SBERT 
conda create -n openobj python=3.8.15
conda activate openobj
pip install functorch==0.2.0
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

æ³¨æ„äº‹é¡¹ï¼š
1. functorch 0.2.0 ä¾èµ– PyTorch 1.xï¼Œä¸ PyTorch 2.x ä¸å…¼å®¹ï¼Œå› æ­¤é¦–å…ˆå•ç‹¬å®‰è£… functorch 0.2.0ï¼Œå†è¦†ç›–å®‰è£… PyTorch 2.x
2. requirements.txt ä¸­å®‰è£…äº† spacyï¼Œä½†æ˜¯æ²¡æœ‰å®‰è£…æ¨¡å‹ï¼Œéœ€è¦æ‰‹åŠ¨å®‰è£… `python -m spacy download en_core_web_sm`
3. å®‰è£… TAP éœ€è¦å®‰è£… flash-attnï¼Œè‡ªå·±ç¼–è¯‘è½®å­å¤ªæ…¢ï¼Œä½¿ç”¨[å®˜æ–¹å‘å¸ƒ](https://github.com/Dao-AILab/flash-attention/releases)çš„é¢„ç¼–è¯‘è½®å­ `flash_attn-2.6.3+cu118torch2.0cxx11abiFALSE-cp38-cp38-linux_x86_64.whl`
4. README ä¸­æ²¡è¯´ä½†æ˜¯è¦ä¸‹è½½ [Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything) å’Œé¢„è®­ç»ƒæ¨¡å‹ `sam_vit_h_4b8939.pth`
5. ä» PyTorch 2.0 å¼€å§‹ï¼Œfunctorch é€æ¸å¼ƒç”¨ï¼Œå‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://pytorch.ac.cn/docs/stable/func.migrating.html)

## 2 è¿è¡Œ

1. Object Segmentation and Understandingï¼šä»å½©è‰²å›¾åƒä¸­è¯†åˆ«å’Œç†è§£å¯¹è±¡å®ä¾‹ï¼ŒåŒ…æ‹¬ CropFormerã€TAP å’Œ CLIP
```shell
python3 maskclustering/mask_gen.py  --input data/vmap/room_0/imap/00/rgb/*.png --input_depth data/vmap/room_0/imap/00/depth/*.png --output results/room_0/mask/ --opts MODEL.WEIGHTS ../OpenObj_third_parties/CropFormer_hornet_3x_03823a.pth 
```
2. Mask Clusteringï¼šç¡®ä¿è·¨å¸§çš„å¯¹è±¡å…³è”ä¸€è‡´
```shell
python3 maskclustering/mask_graph.py --config_file maskclustering/config/room_0.yaml --input_mask results/room_0/mask/mask_init_all.pkl --input_depth data/vmap/room_0/imap/00/depth/*.png --input_pose  data/vmap/room_0/imap/00/traj_w_c.txt --output_graph results/room_0/mask/graph/ --input_rgb data/vmap/room_0/imap/00/rgb/*.png --output_dir data/vmap/room_0/imap/00/ --input_semantic data/vmap/room_0/imap/00/semantic_class/*.png 
```
3. Part-level Fine-Grained Feature Extractionï¼šåŒºåˆ†é›¶ä»¶å¹¶æå–å…¶è§†è§‰ç‰¹å¾
```shell
python partlevel/sam_clip_dir.py --input_image data/vmap/room_0/imap/00/rgb/*.png --output_dir data/vmap/room_0/imap/00/partlevel --down_sample 5
```
4. NeRF Rendering and Trainingï¼šä¸ºæ‰€æœ‰å¯¹è±¡è¿›è¡Œ NeRF è®­ç»ƒ
```shell
python objnerf/train.py --config objnerf/configs/Replica/room_0.json --logdir results/room_0
```
5. Visualizationï¼š
```shell
# ç”Ÿæˆç”¨äºå¯è§†åŒ–çš„æ–‡ä»¶
python visualization/gen_map_vis.py --scene_name room_0 --dataset_name Replica
# ä½¿ç”¨å¯è§†åŒ–æ–‡ä»¶è¿›è¡Œäº¤äº’
python visualization/vis_interaction.py --scene_name room_0 --dataset_name Replica --is_partcolor
```

------

<p align="center">
<h1 align="center"><strong> OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with Fine-Grained Understanding</strong></h1>
</p>



<p align="center">
  <a href="https://openobj.github.io/" target='_blank'>
    <img src="https://img.shields.io/badge/Project-ğŸ‘”-green?">
  </a> 
  
  <a href="https://arxiv.org/pdf/2406.08009412" target='_blank'>
    <img src="https://img.shields.io/badge/Paper-ğŸ“–-blue?">
  </a> 
  
  <a href="https://youtu.be/BeUdxrjItDE" target='_blank'>
    <img src="https://img.shields.io/badge/Video-ğŸ“¹-red?">
  </a> 
</p>


 ## ğŸ   Abstract
In recent years, there has been a surge of interest in open-vocabulary 3D scene reconstruction facilitated by visual language models (VLMs), which showcase remarkable capabilities in open-set retrieval. However, existing methods face some limitations: they either focus on learning point-wise features, resulting in blurry semantic understanding, or solely tackle object-level reconstruction, thereby overlooking the intricate details of the object's interior. To address these challenges, we introduce OpenObj, an innovative approach to build open-vocabulary object-level Neural Radiance Fields (NeRF) with fine-grained understanding. In essence, OpenObj establishes a robust framework for efficient and watertight scene modeling and comprehension at the object-level. Moreover, we incorporate part-level features into the neural fields, enabling a nuanced representation of object interiors. This approach captures object-level instances while maintaining a fine-grained understanding. The results on multiple datasets demonstrate that OpenObj achieves superior performance in zero-shot semantic segmentation and retrieval tasks. Additionally, OpenObj supports real-world robotics tasks at multiple scales, including global movement and local manipulation. connectivity to construct a hierarchical graph. Validation results from public dataset SemanticKITTI demonstrate that, OpenGraph achieves the highest segmentation and query accuracy.
 
<img src="https://github.com/BIT-DYN/OpenObj/blob/main/poster.jpg">


## ğŸ›   Install

### Install the required libraries
Use conda to install the required environment. To avoid problems, it is recommended to follow the instructions below to set up the environment.


```bash
conda env create -f environment.yml
```

###  Install CropFormer Model
Follow the [instructions](https://github.com/qqlu/Entity/blob/main/Entityv2/README.md) to install the CropFormer model and download the pretrained weights [CropFormer_hornet_3x](https://huggingface.co/datasets/qqlu1992/Adobe_EntitySeg/tree/main/CropFormer_model/Entity_Segmentation/CropFormer_hornet_3x).

###  Install TAP Model
Follow the [instructions](https://github.com/baaivision/tokenize-anything?tab=readme-ov-file#installation) to install the TAP model and download the pretrained weights [here](https://github.com/baaivision/tokenize-anything?tab=readme-ov-file#models).


###  Install SBERT ModelD
```bash
pip install -U sentence-transformers
```
Download pretrained weights
```bash
git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
```


### Clone this repo

```bash
git clone https://github.com/BIT-DYN/OpenObj
cd OpenObj
```


## ğŸ“Š Prepare dataset
OpenGraph has completed validation on Replica (as same with [vMap](https://github.com/kxhit/vMAP)) and Scannet. 
Please download the following datasets.

* [Replica Demo](https://huggingface.co/datasets/kxic/vMAP/resolve/main/demo_replica_room_0.zip) - Replica Room 0 only for faster experimentation.
* [Replica](https://huggingface.co/datasets/kxic/vMAP/resolve/main/vmap.zip) - All Pre-generated Replica sequences.
* [ScanNet](https://github.com/ScanNet/ScanNet) - Official ScanNet sequences.


## ğŸƒ Run

### Object Segmentation and Understanding
Run the following command to identifie and comprehend object instances from color images.
```bash
cd maskclustering
python3 mask_gen.py  --input /data/dyn/object/vmap/room_0/imap/00/rgb/*.png --input_depth /data/dyn/object/vmap/room_0/imap/00/depth/*.png --output results/room_0/mask/ --opts MODEL.WEIGHTS CropFormer_hornet_3x_03823a.pth 
```
You can see a visualization of the results in the ```results/vis``` folder.

### Mask Clustering
Run the following command to ensure consistent object association across frames.
```bash
python3 mask_graph.py --config_file ./configs/room_0.yaml --input_mask results/room_0/mask/mask_init_all.pkl --input_depth /data/dyn/object/vmap/room_0/imap/00/depth/*.png --input_pose  /data/dyn/object/vmap/room_0/imap/00/traj_w_c.txt --output_graph results/room_0/mask/graph/ --input_rgb /data/dyn/object/vmap/room_0/imap/00/rgb/*.png --output_dir /data/dyn/object/vmap/room_0/imap/00/ --input_semantic /data/dyn/object/vmap/room_0/imap/00/semantic_class/*.png 
```
You can see a visualization of the results in the ```results/graph``` folder.
And this will generate some folders (```class_our/```  ```instance_our/```) and documents (```object_clipfeat.pkl``` ```object_capfeat.pkl``` ```object_caption.pkl```) in the data directory, which are necessary for the follow-up process. 


### Part-level Fine-Grained Feature Extraction
Run the following command to distinguish parts and extracts their visual features.
```bash
cd ../partlevel
python sam_clip_dir.py --input_image /data/dyn/object/vmap/room_0/imap/00/rgb/*.png --output_dir /data/dyn/object/vmap/room_0/imap/00/partlevel --down_sample 5
```
This will generate a folder (```partlevel/```) in the data directory, which is necessary for the follow-up process. 


### NeRF Rendering and Training
Run the following command to vectorize the training of NeRFs for all objects.
```bash
cd ../nerf
python train.py --config ./configs/Replica/room_0.json --logdir results/room_0
```
This will generate a folder (```ckpt/```) in the result directory containing the network parameters for all objects.

###  visulization
Run the following command to generate the vis documents.
```bash
cd ../nerf
python gen_map_vis.py --scene_name room_0 --dataset_name Replica
```
Interactions can be made using our visualization files.
```bash
cd ../nerf
python vis_interaction.py --scene_name room_0 --dataset_name Replica --is_partcolor
```


Then in the open3d visualizer window, you can use the following key callbacks to change the visualization.

Press ```C``` to toggle the ceiling.

Press ```S``` to color the meshes by the object class. 

Press ```R``` to color the meshes by RGB.

Press ```I``` to color the meshes by object instance ID.

Press ```O``` to color the meshes by part-level feature.

Press ```F``` and type object text and num in the terminal, and the meshes will be colored by the similarity.

Press ```P``` and type object text and num and part text in the terminal, and the meshes will be colored by the similarity.


## ğŸ”— Citation

If you find our work helpful, please cite:

```bibtex
@article{openobj,
  title={OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with Fine-Grained Understanding},
  author={Deng, Yinan and Wang, Jiahui and Zhao, Jingyu and Dou, Jianyu and Yang, Yi and Yue, Yufeng},
  journal={arXiv preprint arXiv:2406.08009},
  year={2024}
}
```

## ğŸ‘ Acknowledgements
We would like to express our gratitude to the open-source projects and their contributors [vMap](https://github.com/kxhit/vMAP). 
Their valuable work has greatly contributed to the development of our codebase.
